{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Deep Learning Seminar 1</h1>\n",
    "\n",
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2 align=\"center\">What was at Lecture?</h2>\n",
    "\n",
    "- Image Classification \n",
    "\n",
    "<img src=\"img/img-clf.png\" width=\"600\">\n",
    "\n",
    "- Linear Models (Что делает линейная модель простым языком)\n",
    "\n",
    "<img src=\"img/lm.png\" width=\"600\">\n",
    "<img src=\"img/lm-int.png\" width=\"600\">\n",
    "\n",
    "- Fully Connected Neural Nets\n",
    "\n",
    "<img src=\"img/fc-net.png\" width=\"600\">\n",
    "\n",
    "- Convolution Neural Nets (Какая мотивация при переходе к сверткам?)\n",
    "\n",
    "<img src=\"img/conv.png\" width=\"600\">\n",
    "\n",
    "- Зачем нужен backprop? \n",
    "\n",
    "<img src=\"img/bp.png\" width=\"600\">\n",
    "\n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "\n",
    "- Что нужно накрутить на SGD чтобы получить хорошие методы стах оптимизации?\n",
    "\n",
    "<img src=\"img/adam.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"img/loss.png\" width=\"300\">\n",
    "<img src=\"img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    N = x.shape[0]\n",
    "    eps = 1e-8\n",
    "    loss = -np.sum(np.log(probs[np.arange(N), y] + eps)) / N\n",
    "\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      "1.14194659231\n"
     ]
    }
   ],
   "source": [
    "print 'loss is a scalar\\n', loss(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      "[ 0.03491482  0.0333956  -0.06831042  0.03047965  0.04580841 -0.07628807\n",
      "  0.02213528 -0.07446436  0.05232907  0.03530645 -0.06458656  0.02928011\n",
      "  0.04054461 -0.06266242  0.02211781  0.03118431 -0.0703896   0.03920529\n",
      "  0.0302208  -0.07148604  0.04126525 -0.05965338  0.03647996  0.02317342\n",
      " -0.06878958  0.02024747  0.04854211  0.02234013  0.05394509 -0.07628522]\n"
     ]
    }
   ],
   "source": [
    "print 'gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 3.62557041388e-08\n"
     ]
    }
   ],
   "source": [
    "print 'difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    N = x.shape[0]\n",
    "    _x = x.reshape((N, -1)) # reshape the input into rows\n",
    "    #print _x\n",
    "    out = _x.dot(w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76984946819e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print 'Testing affine_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:]) # D = d_1 * ... * d_k\n",
    "    x2 = x.reshape((N, D))\n",
    "\n",
    "    dx2 = np.dot(dout, w.T)\n",
    "    dw = np.dot(x2.T, dout)\n",
    "    db = np.dot(dout.T, np.ones(N))\n",
    "\n",
    "    dx = dx2.reshape(x.shape)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  4.50319397853e-10\n",
      "dw error:  6.65351379505e-11\n",
      "db error:  1.31495834311e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                    #\n",
    "    #############################################################################\n",
    "    out = (x > 0) * x # if x_i < 0 then 0 else x_i\n",
    "    #print out\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print 'Testing relu_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    #############################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                   #\n",
    "    #############################################################################\n",
    "    dx = dout * (x >= 0)\n",
    "    dx = dx.reshape(*x.shape)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27562862803e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6efe2c5c50>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6efe0422d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: \t tr_loss 16.58 \t te_loss 16.65 \t te_acc 0.0944444444444\n",
      "epoch 1000: \t tr_loss 3.23 \t te_loss 3.49 \t te_acc 0.701851851852\n",
      "epoch 2000: \t tr_loss 1.80 \t te_loss 2.21 \t te_acc 0.803703703704\n",
      "epoch 3000: \t tr_loss 1.63 \t te_loss 1.26 \t te_acc 0.859259259259\n",
      "epoch 4000: \t tr_loss 1.10 \t te_loss 1.13 \t te_acc 0.875925925926\n",
      "epoch 5000: \t tr_loss 0.47 \t te_loss 0.94 \t te_acc 0.898148148148\n",
      "epoch 6000: \t tr_loss 0.77 \t te_loss 0.92 \t te_acc 0.901851851852\n",
      "epoch 7000: \t tr_loss 0.64 \t te_loss 0.97 \t te_acc 0.905555555556\n",
      "epoch 8000: \t tr_loss 0.48 \t te_loss 0.83 \t te_acc 0.905555555556\n",
      "epoch 9000: \t tr_loss 0.10 \t te_loss 0.98 \t te_acc 0.9\n",
      "epoch 10000: \t tr_loss 0.43 \t te_loss 0.96 \t te_acc 0.901851851852\n",
      "epoch 11000: \t tr_loss 0.77 \t te_loss 0.96 \t te_acc 0.914814814815\n",
      "epoch 12000: \t tr_loss 0.10 \t te_loss 0.68 \t te_acc 0.924074074074\n",
      "epoch 13000: \t tr_loss 0.87 \t te_loss 0.83 \t te_acc 0.909259259259\n",
      "epoch 14000: \t tr_loss 0.30 \t te_loss 0.68 \t te_acc 0.92037037037\n",
      "epoch 15000: \t tr_loss 0.14 \t te_loss 0.78 \t te_acc 0.911111111111\n",
      "epoch 16000: \t tr_loss 0.10 \t te_loss 0.67 \t te_acc 0.92037037037\n",
      "epoch 17000: \t tr_loss 0.24 \t te_loss 0.68 \t te_acc 0.92037037037\n",
      "epoch 18000: \t tr_loss 0.09 \t te_loss 0.67 \t te_acc 0.918518518519\n",
      "epoch 19000: \t tr_loss 0.20 \t te_loss 0.66 \t te_acc 0.927777777778\n",
      "epoch 20000: \t tr_loss 0.25 \t te_loss 0.75 \t te_acc 0.916666666667\n",
      "epoch 21000: \t tr_loss 0.11 \t te_loss 0.74 \t te_acc 0.918518518519\n",
      "epoch 22000: \t tr_loss 0.07 \t te_loss 0.72 \t te_acc 0.911111111111\n",
      "epoch 23000: \t tr_loss 0.09 \t te_loss 0.66 \t te_acc 0.922222222222\n",
      "epoch 24000: \t tr_loss 0.02 \t te_loss 0.65 \t te_acc 0.92037037037\n",
      "epoch 25000: \t tr_loss 0.06 \t te_loss 0.65 \t te_acc 0.925925925926\n",
      "epoch 26000: \t tr_loss 0.05 \t te_loss 0.65 \t te_acc 0.927777777778\n",
      "epoch 27000: \t tr_loss 0.05 \t te_loss 0.66 \t te_acc 0.92962962963\n",
      "epoch 28000: \t tr_loss 0.13 \t te_loss 0.65 \t te_acc 0.92037037037\n",
      "epoch 29000: \t tr_loss 0.33 \t te_loss 0.66 \t te_acc 0.925925925926\n",
      "epoch 30000: \t tr_loss 0.03 \t te_loss 0.62 \t te_acc 0.92962962963\n",
      "epoch 31000: \t tr_loss 0.11 \t te_loss 0.66 \t te_acc 0.924074074074\n",
      "epoch 32000: \t tr_loss 0.30 \t te_loss 1.00 \t te_acc 0.9\n",
      "epoch 33000: \t tr_loss 0.02 \t te_loss 0.62 \t te_acc 0.927777777778\n",
      "epoch 34000: \t tr_loss 0.04 \t te_loss 0.65 \t te_acc 0.92962962963\n",
      "epoch 35000: \t tr_loss 0.43 \t te_loss 0.90 \t te_acc 0.901851851852\n",
      "epoch 36000: \t tr_loss 0.19 \t te_loss 0.70 \t te_acc 0.922222222222\n",
      "epoch 37000: \t tr_loss 0.01 \t te_loss 0.65 \t te_acc 0.927777777778\n",
      "epoch 38000: \t tr_loss 0.01 \t te_loss 0.65 \t te_acc 0.92962962963\n",
      "epoch 39000: \t tr_loss 0.01 \t te_loss 0.65 \t te_acc 0.925925925926\n",
      "epoch 40000: \t tr_loss 0.03 \t te_loss 0.64 \t te_acc 0.925925925926\n",
      "epoch 41000: \t tr_loss 0.17 \t te_loss 0.78 \t te_acc 0.911111111111\n",
      "epoch 42000: \t tr_loss 0.00 \t te_loss 0.61 \t te_acc 0.931481481481\n",
      "epoch 43000: \t tr_loss 0.01 \t te_loss 0.63 \t te_acc 0.927777777778\n",
      "epoch 44000: \t tr_loss 0.07 \t te_loss 0.61 \t te_acc 0.931481481481\n",
      "epoch 45000: \t tr_loss 0.01 \t te_loss 0.66 \t te_acc 0.925925925926\n",
      "epoch 46000: \t tr_loss 0.02 \t te_loss 0.64 \t te_acc 0.933333333333\n",
      "epoch 47000: \t tr_loss 0.00 \t te_loss 0.64 \t te_acc 0.925925925926\n",
      "epoch 48000: \t tr_loss 0.00 \t te_loss 0.62 \t te_acc 0.931481481481\n",
      "epoch 49000: \t tr_loss 0.01 \t te_loss 0.62 \t te_acc 0.935185185185\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.random((64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.random((100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    \n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    # Backward Pass\n",
    "    dx, dW2, db2 = affine_backward(dx, cache3)\n",
    "    dx = relu_backward(dx, cache2)\n",
    "    dx, dW1, db1 = affine_backward(dx, cache1)\n",
    "    \n",
    "    # Updates\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    \n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    te_loss, dx = softmax_loss(out3, y_test)         # Loss Layer\n",
    "    \n",
    "    # Predict\n",
    "    probs = np.exp(out3 - np.max(out3, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch %s:' % i, \n",
    "        print '\\t tr_loss %.2f' % tr_loss,\n",
    "        print '\\t te_loss %.2f' % te_loss,\n",
    "        print '\\t te_acc %s' % accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2 align=\"center\">What is the challenge? </h2>\n",
    "\n",
    "You will see in Assignment 1:\n",
    "- more layers and architectures (Dropout, Convolution, Pooling)\n",
    "- optimization (Momentum, Adam)\n",
    "- weight initialization \n",
    "- data augmentation \n",
    "- ...\n",
    "\n",
    "<img src=\"img/rw.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
