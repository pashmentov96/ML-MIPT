\documentclass[11pt]{article}

\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathptmx} 

\title{\textbf{Машинное обучение.\\ Задание 2}}
\author{Пашментов Никита}
\date{}
\begin{document}

\maketitle

\section{Ответы в листьях регрессионного дерева}
Какая стратегия поведения в листьях регрессионного дерева приводит к меньшему матожиданию ошибки по MSE: отвечать средним значением таргета на объектах обучающей выборки, попавших в лист, или отвечать таргетом для случайного объекта из листа (считая все объекты равновероятными)?

\subsection{Решение}
$\hat{y} = \underset{y}{\arg\max}{\big(\prod\limits_{i = 1}^{n}{P(x^{(i)} \mid y)P(y)}}\big) = \underset{y}{\arg\max}{\big(\prod\limits_{i = 1}^{n}{P(x^{(i)} \mid y)}}\big)$\\
Будем искать максимум прологарифмированной функции:\\
$\hat{y} = \underset{y}{\arg\max}{\big(\sum\limits_{i = 1}^{n}{\ln{(P(x^{(i)} \mid y))}}}\big)$\\
Подставим плотность из условия:\\
$\hat{y} = \underset{y}{\arg\max}{\big(-\frac{1}{2} \ln{(2\pi\sigma^{2}) - \sum\limits_{i = 1}^{n}{\frac{(x^{(i)} - \mu_{yi})^{2}}{2\sigma^{2}}}} \big)} = \underset{y}{\arg\min}{\sum\limits_{i = 1}^{n}{(x^{(i)} - \mu_{yi}) ^ {2}}}$\\
Нетрудно заметить, что:\\
$\hat{y} = \underset{y}{\arg\min}{(\rho(x, \mu_y))}$

\section{Линейные модели в деревьях}
Одна из частых идей - попытаться улучшить регрессионное дерево, выдавая вместо константных ответов в листьях ответ линейной регрессии, обученной на объектах из этого листа.
Как правило такая стратегия не дает никакого ощутимого выигрыша. Попробуйте объяснить, почему? Как стоит модифицировать построение разбиений в дереве по MSE, чтобы при
разбиении получались множества, на которых линейные модели должны работать неплохо?

\subsection{Решение}
$E_N = P(y_n = 1 \mid x_n)P(0\mid x) + P(y_n = 0 \mid x_n)P(1\mid x) \simeq 2P(1 \mid x)P(0 \mid x)$ (по непрерывности $P(y \mid x)$)\\
$2P(1 \mid x)P(0 \mid x) \leq 2\min\{P(1 \mid x), P(0 \mid x)\}$\\
Следовательно:\\
$E_B \leq 2E_N$ - ч.т.д.

\section{Unsupervised decision tree}
Unsupervised решающие деревья можно было бы применить для кластеризации выборки или
оценки плотности, но проблема построения таких деревьев заключается в введении меры
информативности. В одной статье предлагался следующий подход - оценивать энтропию
множества S по формуле:
\begin{displaymath}
H(S) = \frac{1}{2}\ln{((2\pi e)^{n} |\Sigma)|}
\end{displaymath}
Здесь \Sigma - оцененная по множеству матрица ковариаций. Т.е. не имея других сведений, в
предложенном подходе мы по умолчанию считаем, что скопления точек можно приближенно
считать распределенными нормально. Убедитесь, что это выражение в самом деле задает
энтропию многомерного нормального распределения.
\end{document}
