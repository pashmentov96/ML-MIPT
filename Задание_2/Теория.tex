\documentclass[11pt]{article}

\usepackage{mathtext}
\usepackage[T1,T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathptmx} 

\title{\textbf{Машинное обучение.\\ Задание 2}}
\author{Пашментов Никита}
\date{}
\begin{document}

\maketitle

\section{Ответы в листьях регрессионного дерева}
Какая стратегия поведения в листьях регрессионного дерева приводит к меньшему матожиданию ошибки по MSE: отвечать средним значением таргета на объектах обучающей выборки, попавших в лист, или отвечать таргетом для случайного объекта из листа (считая все объекты равновероятными)?

\subsection{Решение}
Пусть в лист попали объекты $X_i$ с значениями $y_i$. \\
$\xi = \frac{1}{n} \sum \limits_{i = 1}^{n}{y_i}$\\
$E \sum \limits _{i = 1} ^ {n} {(y_i - \xi) ^ 2} = E \sum \limits _{i = 1} ^ {n} {y_i ^ 2} - E \sum \limits _{i = 1} ^ {n} {2 y_i \xi} +  E \sum \limits _{i = 1} ^ {n} {\xi ^ 2}$\\
$\eta = y_{\theta}$, где ${\theta}$ - дискретное равномерное распределение.\\
$E \sum \limits _{i = 1} ^ {n} {(y_i - \eta) ^ 2} = E \sum \limits _{i = 1} ^ {n} {y_i ^ 2} - E \sum \limits _{i = 1} ^ {n} {2 y_i \eta} +  E \sum \limits _{i = 1} ^ {n} {\eta ^ 2}$\\
Утверждение: $E \sum \limits _{i = 1} ^ {n} {(y_i - \xi) ^ 2} \leq E \sum \limits _{i = 1} ^ {n} {(y_i - \eta) ^ 2}$\\
Т.к. $E \xi = E \eta = \frac{1}{n} \sum \limits _{i = 1} ^ {n} {y_i}$, то $E \sum \limits _{i = 1} ^ {n} {2 y_i \xi} = E \sum \limits _{i = 1} ^ {n} {2 y_i \eta}$. Следовательно, утверждение эквивалентно следующему: $E\xi ^ 2 \leq E \eta ^ 2$\\
$E \eta ^ 2 \geq (E \eta) ^ 2 = (E \xi) ^ 2 = E \xi ^ 2$\\
$\boldmath{Ответ}$: Лучше отвечать средним значением таргета.

\section{Линейные модели в деревьях}
Одна из частых идей - попытаться улучшить регрессионное дерево, выдавая вместо константных ответов в листьях ответ линейной регрессии, обученной на объектах из этого листа.
Как правило такая стратегия не дает никакого ощутимого выигрыша. Попробуйте объяснить, почему? Как стоит модифицировать построение разбиений в дереве по MSE, чтобы при
разбиении получались множества, на которых линейные модели должны работать неплохо?

\subsection{Решение}
Критерий построения разбиений в регрессионном дереве никак не учитывает то, насколько в
каждой из дочерних ветвей зависимость близка к линейной(можно
взять точки на прямой с большим расстоянием друг от друга). Тогда в качестве меры "хорошести"  множества можно использовать, например, MSE модели $a(x)$, обученной на данном множестве, т.е.:
\begin{displaymath}
F(Q) = \frac{1}{|Q|} \sum \limits _{x_j \in Q} {(y_i - a(x_i)) ^ 2}
\end{displaymath}
И подставить в формулу для критерия разбиения $F(Q) - \frac{|L|}{|Q|}F(L) - \frac{|R|}{|Q|}F(R)$


\section{Unsupervised decision tree}
Unsupervised решающие деревья можно было бы применить для кластеризации выборки или
оценки плотности, но проблема построения таких деревьев заключается в введении меры
информативности. В одной статье предлагался следующий подход - оценивать энтропию
множества S по формуле:
\begin{displaymath}
H(S) = \frac{1}{2}\ln{((2\pi e)^{n} |\Sigma|)}
\end{displaymath}
Здесь $\Sigma$ - оцененная по множеству матрица ковариаций. Т.е. не имея других сведений, в
предложенном подходе мы по умолчанию считаем, что скопления точек можно приближенно
считать распределенными нормально. Убедитесь, что это выражение в самом деле задает
энтропию многомерного нормального распределения.

\subsection{Решение}
По определению \\
$H(f) = -\int \limits_{x \in \mathbb{R}^n} f(x) \ln(f(x))dx = -{E}\ln(f(x)) $\\
$ \ln{(\frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}} } e^{ \frac{1}{2} (x - \mu)^T \Sigma^{-1} (x-\mu) })} = -\ln{((2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}})} - \frac{1}{2} (x - \mu)^T |\Sigma|^{-1} (x - \mu)$\\
$E \ln{(f(x))} = -\ln{((2\pi)^{\frac{1}{2}} |\Sigma|^{\frac{1}{2}})} - \frac{1}{2} E (x-\mu)^T |\Sigma|^{-1} (x - \mu)$\\
Где $ \frac{1}{2} E (x-\mu)^T |\Sigma|^{-1} (x - \mu) =  -\frac{1}{2} tr(\Sigma \Sigma^{-1}) = -\frac{1}{2}n$\\
Итого
$ H(f) =  -E\ln {(f(x))}  = \frac{n}{2} + \ln{((2\pi)^{\frac{n}{2}})} |\Sigma|^{\frac{1}{2}} = \frac{n}{2} + \frac{1}{2} \ln {((2\pi)^n |\Sigma|)} = \frac{1}{2} \ln{((2\pi e)^n |\Sigma|)} $

\end{document}
